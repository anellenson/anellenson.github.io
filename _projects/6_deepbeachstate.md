---
layout: page
title: Identifying Sandbars in Coastal Images with a Convolutional Neural Network
description: Automating the identification of sandbars in imagery for different beaches
img: assets/img/beachstates/saliencymaps.jpg
importance: 6
category: PhD
related_publications: true
---

<h3> How can we leverage decades worth of coastal imagery to get more information about sandbar shape and behavior? </h3>

In this research, I developed a deep learning framework  (Convolutional Neural Network) to identify sandbars in the coastal (nearshore) zone. 

Sandbars play a role in recreation, navigation, erosion and pollutant transport. They are landforms in the coast near the beach that determine wave breaking and circulation patterns. Feedback between sediment transport and waves - and sometimes the waves themselves - will determine what shape the sandbars will take on. The different sandbar categories are shown below, and <a href="https://www.sciencedirect.com/science/article/abs/pii/0025322784900082"> were developed in the 80s </a>. Distinctions between the categories are due to features such as curviness (vs straightness), attachment to the shoreline (vs trough), rip currents existing or not.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/beachstates/duck_beachstates.jpg" title="Beach State Categories" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Day time exposure images of Argus imagery showing sandbars and the different associated categories. The images are shown from a "bird's eye" perspective, where the land is colored dark on the top of the image, and the sandbars are white signatures on the gray water.  
</div>

The sandbar categories readily lend themselves to training a supervised convolutional neural network. Also, a plethora of data from <a href="https://en.wikipedia.org/wiki/Argus_Coastal_Monitoring"> Argus stations </a> around the world was also readily available as training and testing data. I used data from Duck, North Carolina and Narrabeen, Australia.

However, a challenge arose in that the sandbars didn't always fall neatly into distinct categories, leading to the question of how to assess CNN accuracy for a method that requires distinct labelling. To establish a standard of baseline accuracy, the images were manually labelled and the "inter-labeller agreement" f1 score was taken as the accuracy standard, shown below. 

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/beachstates/labelvscnn.jpg" title="Inter-labeller Agreement" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Interlabeller agreement that set the accuracy basline for each category. 
</div>

The CNN was trained on images that I manually picked out and labelled, trying to find the images that fit within most discrete categories within the entire dataset. I used image augmentation to increase the training dataset size.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/beachstates/CNNtrainingmethod.jpg" title="CNN Training Method" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Development of the CNN including training and testing flows.
</div>

We also performed several experiments to determine if the CNN was transferable to other locations: 
<li> CNN trained at one location and tested at the same location
<li> CNN trained at one location and tested at separate location
<li> CNN trained at both locations and tested at both locations

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/beachstates/fscore.jpg" title="F Score" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    F-score for the three CNN experiments. The color designates where the CNN was trained, and the different columns are where the CNN was tested.
</div>

Overall, the CNN performed similar to the baseline accuracy standard. The CNN did not perform the same at both locations; the sandbars were more recognizable at one location compared to the other location. Also, the CNN didn't transfer equally; it transferred better to one location than the other. 

Finally, transparency of the CNN decision making was assessed, trying to answer the question - how can we trust the CNN is looking at the right things? Saliency maps, generated by back propagating category weights through the CNN, show which features are most important to the CNN for making decision.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/beachstates/saliencymaps.jpg" title="Saliency Maps" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Saliency maps where the colored pixels show which features the CNN were most relevant in the decision making. 
</div>

To read more or see the code development, check out the paper {% cite ellenson2020beach %} and the github repo. 

<div class="repositories d-flex flex-wrap flex-md-row flex-column justify-content-between align-items-center">
    {% include repository/repo.liquid repository='anellenson/DeepBeachState' %}
</div>
